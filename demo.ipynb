{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Notebook\n",
    "This notebook provides a simple demonstration of using DeepSeek Chat (Deepseek-v3) API using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('DEEPSEEK_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and split text\n",
    "loader = TextLoader(\"data/demo_data.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Generate embeddings & store in ChromaDB\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma.from_documents(docs, embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the phenomenon that allows a system to exist in multiple configurations?\"\n",
    "retrieved_docs = vector_db.similarity_search(query, k=3)\n",
    "retrieved_texts = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Response with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The phenomenon that allows a system to exist in multiple configurations is **superposition**. In quantum mechanics, superposition refers to the state in which a quantum particle or system can represent not just one possibility, but a combination of multiple possibilities simultaneously. This is a fundamental principle that enables quantum computers to explore many potential solutions at once, unlike classical computers, which are deterministic and process one solution at a time.\n"
     ]
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",  # or \"deepseek-reasoner\" for DeepSeek-R1\n",
    "    temperature=0.7,\n",
    "    max_tokens=150,\n",
    "    timeout=30,\n",
    "    max_retries=2,\n",
    "    # api_key=api_key,\n",
    "    # user='vtohal@gmail.com'\n",
    ")\n",
    "\n",
    "response = llm.invoke(f\"Answer using this context:\\n{retrieved_texts}\\n\\nQuestion: {query}\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "install_libraries = False # Set to true if required\n",
    "if install_libraries:\n",
    "    print('Installing libraries.')\n",
    "    %pip install spacy yake scikit-learn keybert\n",
    "    \n",
    "    import spacy\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Job description\n",
    "use_sample_jd = False\n",
    "if use_sample_jd:\n",
    "    job_description = \"\"\"We are looking for a Machine Learning Engineer with experience in Python, TensorFlow, \n",
    "and cloud computing (AWS/GCP). The ideal candidate should have a strong background in deep learning and \n",
    "natural language processing (NLP).\"\"\"\n",
    "else:\n",
    "    job_description = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords using LLM\n",
    "prompt_extract_kw = \"Extract keywords related to job description that would help in matching a potential resume to it. Return just a list of comma separated keywords\"\n",
    "llm_keywords_response = llm.invoke(f\"{prompt_extract_kw}\\n\\nJob Description:\\n{job_description}\")\n",
    "llm_keywords = [kw.strip() for kw in llm_keywords_response.content.split(',')]\n",
    "print(\"LLM extracted keywords:\")\n",
    "print(llm_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the desire', 'excellence', 'Publication record', 'Familiarity', 'Pandas', 'Collaborate', 'client-centric solutions', 'Required Skills / Expertise  We', 'ARIMA', 'software', 'the application', 'machine learning', 'Required Qualifications', 'Required Skills / Expertise', 'patterns', 'a new applied domain', 'peer-reviewed academic conferences', 'Desire', 'ownership', 'Engineering Research', 'a problem', 'relevant journals', 'project experience', 'Key Responsibilities', 'efficiency', 'TensorFlow', '(M.Sc/Ph.D', 'programming language', 'Canada Preferred Qualifications', 'Canada', 'Solid understanding', 'minimum viable products', 'GRUs', 'Design', 'machine intelligence Intellectual curiosity', 'Optimize ML pipelines', 'A positive attitude', 'time series data', 'classical time series models', 'Publication', 'a graduate level program', 'Completion', 'ML/DL', 'scalability', 'regular client meetings', 'Facebook Prophet', 'machine', 'Python', 'a talented and enthusiastic individual', 'ML', 'Optimize ML', 'PyTorch', 'Non-Technical Requirements', 'real-time processing capabilities', 'MVPs', 'technologies', 'related ML frameworks', 'experience', 'train', 'LSTMs', 'ML/DL models', 'solid knowledge', 'reports', 'time series classification or anatomy detection methods', 'presentations', 'new things', 'historical datasets', 'technical concepts', 'toolkits', 'cross-functional teams', 'applications', 'RNNs', 'exploratory data analysis', 'best practices   Non-Technical Requirements', 'techniques', 'deep learning techniques', 'libraries', 'Computing Science', 'preprocess', 'anomalies']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_keywords_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    keywords = set()\n",
    "\n",
    "    # Extract noun chunks (phrases like \"machine learning engineer\")\n",
    "    keywords.update(chunk.text for chunk in doc.noun_chunks)\n",
    "\n",
    "    # Extract Named Entities (like \"Python\", \"Google Cloud\", etc.)\n",
    "    keywords.update(ent.text for ent in doc.ents)\n",
    "\n",
    "    return list(keywords)\n",
    "\n",
    "\n",
    "\n",
    "keywords = extract_keywords_spacy(job_description)\n",
    "print(keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Expertise We â€™re', 'time series data', 'time series', 'time series models', 'Canada Preferred Qualifications', 'capabilities Required Qualifications', 'series data', 'classical time series', 'experience working', 'time series classification', 'project experience working', 'machine learning Familiarity', 'solid knowledge', 'Required Qualifications', 'Expertise', 'time', 'enthusiastic individual', 'machine learning', 'series', 'skills Interdisciplinary team']\n"
     ]
    }
   ],
   "source": [
    "from yake import KeywordExtractor\n",
    "\n",
    "def extract_keywords_yake(text, top_k=20):\n",
    "    extractor = KeywordExtractor(lan=\"en\", n=3, top=top_k)\n",
    "    keywords = extractor.extract_keywords(text)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "keywords = extract_keywords_yake(job_description)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['client' 'data' 'desire' 'engineering' 'experience' 'learn' 'learning'\n",
      " 'machine' 'ml' 'models' 'new' 'qualifications' 'required' 'series'\n",
      " 'solid' 'technical' 'techniques' 'time' 'understanding' 'working']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_keywords_tfidf(text, top_k=10):\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=top_k)\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "keywords = extract_keywords_tfidf(job_description, top_k=20)\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['skills expertise looking', 'required skills expertise', 'skills expertise', 'capabilities required qualifications', 'required skills', 'expertise looking talented', 'science ml engineering', 'expertise looking', 'ml engineering', 'ml pipelines', 'learn tensorflow', 'required qualifications', 'expertise', 'time processing capabilities', 'related ml frameworks', 'machine learning experience', 'qualifications', 'skills interdisciplinary team', 'processing capabilities required', 'ml engineering research']\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "def extract_keywords_bert(text, top_n=10):\n",
    "    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1,3), stop_words=\"english\", top_n=top_n)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "print(extract_keywords_bert(job_description, top_n=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
